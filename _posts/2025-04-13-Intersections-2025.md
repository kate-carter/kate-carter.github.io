## Intersections Spring 2025
#
April 18th, 2025, Case Western Reserve University.

Poster: *Automated Co-Speech Gesture Recognition using Pre-trained Visual Transformers*

### Abstract
#
Communication is multimodal, yet most research into human communication has been on spoken or textual language due to the abundance of resources for creating and processing text-based datasets. Other aspects of multimodal communication are understudied due to a lack of tools to create and analyze large datasets of audiovisual data. In the age of data science, new tools for multimodal data analysis have emerged. These will enable advancements in multimodal communications research if properly adapted for linguistic analysis. This project investigates and qualitatively assesses machine learning models for co-speech gesture annotation. In future work, the models identified will then be trained on a linguistically salient dataset of co-speech gesture classes. If they perform well enough, one will be integrated into the Distributed Little Red Hen Labâ€™s co-speech gesture annotation pipeline.  Two models, ViViT and TubeViT, have been identified to perform well on video datasets with similar hand focus and granularity as would be required for linguistic gesture analysis.  This paper outlines the criteria and testing the models were assessed with that led to this conclusion. It also lays out the road map of the next steps in the project.  First,  the performance of these pre-trained models on co-speech gesture annotation tasks will be quantitatively evaluated. Next, a training dataset for this task will be created to fine-tune the pre-trained models for this task. Finally, TubeViT and ViViT will be quantitatively assessed after fine-tuning on a task-specific dataset to determine what processing will have to occur around their input and output to successfully integrate one into the Red Hen co-speech gesture annotation pipeline.  

#
![Intersections Poster Image](Intersections_Poster.png)
*
![Red Hen](https://github.com/RedHenLab)
